{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8da8d5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import gymnasium as gym\n",
    "import snntorch as snn\n",
    "from snntorch import functional as SF\n",
    "from snntorch import spikeplot as splt\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv, VecFrameStack\n",
    "\n",
    "from spikingjelly.clock_driven import ann2snn, functional\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14cfb6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the ANN model (update for your environment)\n",
    "ann_model_path = \"/PongNoFrameskip-v4.zip\"\n",
    "\n",
    "# Determine device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create Atari Pong evaluation environment\n",
    "env = make_atari_env(\"PongNoFrameskip-v4\", n_envs=1, seed=0)\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "video_folder = '/Volumes/export/isn/diana/bindsnet/examples/pong/logs/videos/'  # Folder to save videos\n",
    "video_length = 2000  # Length of the recorded video (in timesteps)\n",
    "env = VecVideoRecorder(env, video_folder,\n",
    "                     record_video_trigger=lambda x: x == 0,  # Record starting from the first step\n",
    "                     video_length=video_length,\n",
    "                     name_prefix=f\"PongNoFrameskip-v4-SNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae88c49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk/diana/miniconda3/envs/sbzoo/lib/python3.10/site-packages/stable_baselines3/common/save_util.py:167: UserWarning: Could not deserialize object learning_rate. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: 'bytes' object cannot be interpreted as an integer\n",
      "  warnings.warn(\n",
      "/local_disk/diana/miniconda3/envs/sbzoo/lib/python3.10/site-packages/stable_baselines3/common/save_util.py:167: UserWarning: Could not deserialize object lr_schedule. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: 'bytes' object cannot be interpreted as an integer\n",
      "  warnings.warn(\n",
      "/local_disk/diana/miniconda3/envs/sbzoo/lib/python3.10/site-packages/stable_baselines3/common/save_util.py:167: UserWarning: Could not deserialize object exploration_schedule. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: 'bytes' object cannot be interpreted as an integer\n",
      "  warnings.warn(\n",
      "/local_disk/diana/miniconda3/envs/sbzoo/lib/python3.10/site-packages/stable_baselines3/common/vec_env/patch_gym.py:95: UserWarning: You loaded a model that was trained using OpenAI Gym. We strongly recommend transitioning to Gymnasium by saving that model again.\n",
      "  warnings.warn(\n",
      "/local_disk/diana/miniconda3/envs/sbzoo/lib/python3.10/site-packages/stable_baselines3/common/base_class.py:773: UserWarning: You are probably loading a DQN model saved with SB3 < 2.4.0, we truncated the optimizer state so you can save the model again to avoid issues in the future (see https://github.com/DLR-RM/stable-baselines3/pull/1963 for more info). Original error: loaded state dict contains a parameter group that doesn't match the size of optimizer's group \n",
      "Note: the model should still work fine, this only a warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load models\n",
    "ann_model_path = \"/Volumes/export/isn/diana/rl-baselines3-zoo/logs/dqn/PongNoFrameskip-v4_1/PongNoFrameskip-v4.zip\"\n",
    "ann_model = DQN.load(ann_model_path, custom_objects={\"replay_buffer_class\": None, \"optimize_memory_usage\": False})\n",
    "snn_model = torch.load(\"snn_pong_q_net_full.pt\", weights_only=False, map_location=device)\n",
    "fused_snn = torch.load(\"fused_snn_pong.pt\", weights_only=False, map_location=device)\n",
    "target_snn = torch.load(\"fused_snn_pong.pt\", weights_only=False, map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90c034eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_net.features_extractor.cnn.0.weight    requires_grad=False\n",
      "q_net.features_extractor.cnn.0.bias      requires_grad=False\n",
      "q_net.features_extractor.cnn.2.weight    requires_grad=False\n",
      "q_net.features_extractor.cnn.2.bias      requires_grad=False\n",
      "q_net.features_extractor.cnn.4.weight    requires_grad=False\n",
      "q_net.features_extractor.cnn.4.bias      requires_grad=False\n",
      "q_net.features_extractor.linear.0.weight requires_grad=False\n",
      "q_net.features_extractor.linear.0.bias   requires_grad=False\n",
      "q_net.q_net.0.weight                     requires_grad=True\n",
      "q_net.q_net.0.bias                       requires_grad=True\n",
      "q_net_target.features_extractor.cnn.0.weight requires_grad=False\n",
      "q_net_target.features_extractor.cnn.0.bias requires_grad=False\n",
      "q_net_target.features_extractor.cnn.2.weight requires_grad=False\n",
      "q_net_target.features_extractor.cnn.2.bias requires_grad=False\n",
      "q_net_target.features_extractor.cnn.4.weight requires_grad=False\n",
      "q_net_target.features_extractor.cnn.4.bias requires_grad=False\n",
      "q_net_target.features_extractor.linear.0.weight requires_grad=False\n",
      "q_net_target.features_extractor.linear.0.bias requires_grad=False\n",
      "q_net_target.q_net.0.weight              requires_grad=True\n",
      "q_net_target.q_net.0.bias                requires_grad=True\n"
     ]
    }
   ],
   "source": [
    "# 1) Freeze everything in feature extractor\n",
    "features_extractor = ann_model.policy.q_net.features_extractor\n",
    "for p in features_extractor.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "target_features_extractor = ann_model.policy.q_net_target.features_extractor\n",
    "for p in target_features_extractor.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# 3) Verify\n",
    "for name, p in ann_model.policy.named_parameters():\n",
    "    print(f\"{name:40s} requires_grad={p.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b610f67f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_model = DQN.load(ann_model_path, custom_objects={\"replay_buffer_class\": None, \"optimize_memory_usage\": False})\n",
    "\n",
    "# glorot initialize q network\n",
    "nn.init.xavier_normal_(ann_model.policy.q_net.q_net[0].weight)\n",
    "nn.init.zeros_(ann_model.policy.q_net.q_net[0].bias)\n",
    "nn.init.xavier_normal_(ann_model.policy.q_net_target.q_net[0].weight)\n",
    "nn.init.zeros_(ann_model.policy.q_net_target.q_net[0].bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82d702b",
   "metadata": {},
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f68333a",
   "metadata": {},
   "source": [
    "# finetune ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fd0e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 001  Reward: -18.0  Epsilon: 0.467\n",
      "Episode 002  Reward: -18.0  Epsilon: 0.462\n",
      "Episode 003  Reward: -19.0  Epsilon: 0.457\n",
      "Episode 004  Reward: -21.0  Epsilon: 0.453\n",
      "Episode 005  Reward: -21.0  Epsilon: 0.449\n",
      "Episode 006  Reward: -21.0  Epsilon: 0.445\n",
      "Episode 007  Reward: -20.0  Epsilon: 0.441\n",
      "Episode 008  Reward: -21.0  Epsilon: 0.438\n",
      "Episode 009  Reward: -21.0  Epsilon: 0.434\n",
      "Episode 010  Reward: -21.0  Epsilon: 0.431\n",
      "Saved model at episode 10\n",
      "Episode 011  Reward: -21.0  Epsilon: 0.428\n",
      "Episode 012  Reward: -21.0  Epsilon: 0.424\n",
      "Episode 013  Reward: -21.0  Epsilon: 0.421\n",
      "Episode 014  Reward: -20.0  Epsilon: 0.417\n",
      "Episode 015  Reward: -20.0  Epsilon: 0.414\n",
      "Episode 016  Reward: -21.0  Epsilon: 0.410\n",
      "Episode 017  Reward: -21.0  Epsilon: 0.407\n",
      "Episode 018  Reward: -21.0  Epsilon: 0.404\n",
      "Episode 019  Reward: -21.0  Epsilon: 0.401\n",
      "Episode 020  Reward: -21.0  Epsilon: 0.398\n",
      "Saved model at episode 20\n",
      "Episode 021  Reward: -21.0  Epsilon: 0.395\n",
      "Episode 022  Reward: -20.0  Epsilon: 0.391\n",
      "Episode 023  Reward: -21.0  Epsilon: 0.388\n",
      "Episode 024  Reward: -21.0  Epsilon: 0.385\n",
      "Episode 025  Reward: -20.0  Epsilon: 0.382\n",
      "Episode 026  Reward: -20.0  Epsilon: 0.379\n",
      "Episode 027  Reward: -21.0  Epsilon: 0.376\n",
      "Episode 028  Reward: -21.0  Epsilon: 0.373\n",
      "Episode 029  Reward: -21.0  Epsilon: 0.370\n",
      "Episode 030  Reward: -21.0  Epsilon: 0.367\n",
      "Saved model at episode 30\n",
      "Episode 031  Reward: -21.0  Epsilon: 0.364\n",
      "Episode 032  Reward: -20.0  Epsilon: 0.361\n",
      "Episode 033  Reward: -20.0  Epsilon: 0.359\n",
      "Episode 034  Reward: -21.0  Epsilon: 0.356\n",
      "Episode 035  Reward: -20.0  Epsilon: 0.353\n",
      "Episode 036  Reward: -21.0  Epsilon: 0.350\n",
      "Episode 037  Reward: -21.0  Epsilon: 0.348\n",
      "Episode 038  Reward: -20.0  Epsilon: 0.344\n",
      "Episode 039  Reward: -21.0  Epsilon: 0.342\n",
      "Episode 040  Reward: -21.0  Epsilon: 0.339\n",
      "Saved model at episode 40\n",
      "Episode 041  Reward: -21.0  Epsilon: 0.337\n",
      "Episode 042  Reward: -21.0  Epsilon: 0.334\n",
      "Episode 043  Reward: -21.0  Epsilon: 0.332\n",
      "Episode 044  Reward: -21.0  Epsilon: 0.329\n",
      "Episode 045  Reward: -20.0  Epsilon: 0.326\n",
      "Episode 046  Reward: -21.0  Epsilon: 0.324\n",
      "Episode 047  Reward: -19.0  Epsilon: 0.320\n",
      "Episode 048  Reward: -21.0  Epsilon: 0.318\n",
      "Episode 049  Reward: -21.0  Epsilon: 0.316\n",
      "Episode 050  Reward: -21.0  Epsilon: 0.313\n",
      "Saved model at episode 50\n",
      "Episode 051  Reward: -21.0  Epsilon: 0.311\n",
      "Episode 052  Reward: -20.0  Epsilon: 0.308\n",
      "Episode 053  Reward: -21.0  Epsilon: 0.306\n",
      "Episode 054  Reward: -20.0  Epsilon: 0.303\n",
      "Episode 055  Reward: -20.0  Epsilon: 0.301\n",
      "Episode 056  Reward: -20.0  Epsilon: 0.299\n",
      "Episode 057  Reward: -21.0  Epsilon: 0.296\n",
      "Episode 058  Reward: -21.0  Epsilon: 0.294\n",
      "Episode 059  Reward: -21.0  Epsilon: 0.292\n",
      "Episode 060  Reward: -20.0  Epsilon: 0.290\n",
      "Saved model at episode 60\n",
      "Episode 061  Reward: -21.0  Epsilon: 0.288\n",
      "Episode 062  Reward: -21.0  Epsilon: 0.285\n",
      "Episode 063  Reward: -20.0  Epsilon: 0.283\n",
      "Episode 064  Reward: -21.0  Epsilon: 0.281\n",
      "Episode 065  Reward: -20.0  Epsilon: 0.278\n",
      "Episode 066  Reward: -21.0  Epsilon: 0.276\n",
      "Episode 067  Reward: -21.0  Epsilon: 0.274\n",
      "Episode 068  Reward: -20.0  Epsilon: 0.272\n",
      "Episode 069  Reward: -21.0  Epsilon: 0.270\n",
      "Episode 070  Reward: -19.0  Epsilon: 0.268\n",
      "Saved model at episode 70\n",
      "Episode 071  Reward: -21.0  Epsilon: 0.266\n",
      "Episode 072  Reward: -21.0  Epsilon: 0.264\n",
      "Episode 073  Reward: -21.0  Epsilon: 0.262\n",
      "Episode 074  Reward: -21.0  Epsilon: 0.260\n",
      "Episode 075  Reward: -20.0  Epsilon: 0.258\n",
      "Episode 076  Reward: -20.0  Epsilon: 0.256\n",
      "Episode 077  Reward: -19.0  Epsilon: 0.253\n",
      "Episode 078  Reward: -20.0  Epsilon: 0.251\n",
      "Episode 079  Reward: -20.0  Epsilon: 0.249\n",
      "Episode 080  Reward: -21.0  Epsilon: 0.247\n",
      "Saved model at episode 80\n",
      "Episode 081  Reward: -21.0  Epsilon: 0.245\n",
      "Episode 082  Reward: -21.0  Epsilon: 0.243\n",
      "Episode 083  Reward: -21.0  Epsilon: 0.242\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from spikingjelly.clock_driven import functional as sf_func\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "\n",
    "# ─── Hyperparameters ────────────────────────────────────────────────────────────\n",
    "ENV_ID         = \"PongNoFrameskip-v4\"\n",
    "NUM_EPISODES   = 500\n",
    "GAMMA          = 0.99\n",
    "LR             = 1e-4\n",
    "TARGET_SYNC    = 10       # episodes between syncing target network\n",
    "BUFFER_SIZE    = 100_000\n",
    "BATCH_SIZE     = 32\n",
    "MIN_REPLAY     = 1_000    # start training after this many transitions\n",
    "EPS_START      = 0.245\n",
    "EPS_END        = 0.02\n",
    "EPS_DECAY      = 100_000  # frames over which epsilon decays\n",
    "DEVICE         = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ─── Environment setup ──────────────────────────────────────────────────────────\n",
    "env = make_atari_env(ENV_ID, n_envs=1, seed=0)\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "def unwrap(obs_tuple):\n",
    "    # unwrap VecEnv: obs_tuple is ([frames], infos)\n",
    "    return obs_tuple[0]\n",
    "\n",
    "# ─── Load & clone networks ──────────────────────────────────────────────────────\n",
    "fine_tuned_ann_model_path = \"./ann_q_net_finetuned.pth\" # epsilon = 0.245, 170 episodes\n",
    "fine_tuned_ann_target_model_path = \"./ann_q_net_finetuned_target.pth\" \n",
    "ann_model = torch.load(fine_tuned_ann_model_path, weights_only=False, map_location=DEVICE)\n",
    "ann_model_target = torch.load(fine_tuned_ann_target_model_path, weights_only=False, map_location=DEVICE)\n",
    "# ann_model = DQN.load(ann_model_path, custom_objects={\"replay_buffer_class\": None, \"optimize_memory_usage\": False})\n",
    "# ann_model_target = ann_model.q_net_target\n",
    "# ann_model = ann_model.q_net\n",
    "\n",
    "# ─── Replay buffer ──────────────────────────────────────────────────────────────\n",
    "Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "replay_buffer = deque(maxlen=BUFFER_SIZE)\n",
    "\n",
    "# ─── Optimizer & loss ───────────────────────────────────────────────────────────\n",
    "optimizer = optim.Adam(ann_model.parameters(), lr=LR)\n",
    "criterion = nn.SmoothL1Loss()   # Huber loss\n",
    "\n",
    "# ─── Epsilon schedule ───────────────────────────────────────────────────────────\n",
    "def epsilon_by_frame(frame_idx):\n",
    "    return EPS_END + (EPS_START - EPS_END) * np.exp(-1.0 * frame_idx / EPS_DECAY)\n",
    "\n",
    "# ─── Helper: compute Q‐rates for a batch of observations ─────────────────────────\n",
    "def compute_q_rates(net, obs_batch):\n",
    "    \"\"\"\n",
    "    obs_batch: Tensor of shape (B, H, W, C), values in [0,255]\n",
    "    returns: Tensor of shape (B, action_dim)\n",
    "    \"\"\"\n",
    "    qs = []\n",
    "    for obs in obs_batch:\n",
    "        x = obs.permute(2, 0, 1).unsqueeze(0).to(DEVICE) / 255.0\n",
    "        sf_func.reset_net(net)\n",
    "        qs.append(net(x))\n",
    "    return torch.cat(qs, dim=0)\n",
    "\n",
    "# ─── Pre-fill replay buffer with random play ────────────────────────────────────\n",
    "obs = unwrap(env.reset())\n",
    "for _ in range(MIN_REPLAY):\n",
    "    action = env.action_space.sample()\n",
    "    next_obs, reward, done, _ = env.step([action])\n",
    "    next_obs, reward, done = next_obs[0], reward[0], done[0]\n",
    "    replay_buffer.append(Transition(obs, action, reward, next_obs, done))\n",
    "    obs = next_obs if not done else unwrap(env.reset())\n",
    "\n",
    "# ─── Main training loop ─────────────────────────────────────────────────────────\n",
    "frame_idx = 0\n",
    "for ep in range(1, NUM_EPISODES + 1):\n",
    "    obs = unwrap(env.reset())\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    # reset spiking states\n",
    "    sf_func.reset_net(ann_model)\n",
    "    sf_func.reset_net(ann_model_target)\n",
    "\n",
    "    while not done:\n",
    "        frame_idx += 1\n",
    "        eps = epsilon_by_frame(frame_idx)\n",
    "\n",
    "        # ε-greedy action selection\n",
    "        if random.random() < eps:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_rate = compute_q_rates(ann_model, torch.tensor(obs[None], dtype=torch.float32))\n",
    "            action = q_rate.argmax(dim=1).item()\n",
    "\n",
    "        # step environment\n",
    "        next_obs, reward, done, _ = env.step([action])\n",
    "        next_obs, reward, done = next_obs[0], reward[0], done[0]\n",
    "        replay_buffer.append(Transition(obs, action, reward, next_obs, done))\n",
    "        obs = next_obs\n",
    "        total_reward += reward\n",
    "\n",
    "        # once we have enough samples, perform a training step\n",
    "        if len(replay_buffer) >= MIN_REPLAY:\n",
    "            transitions = random.sample(replay_buffer, BATCH_SIZE)\n",
    "            batch = Transition(*zip(*transitions))\n",
    "\n",
    "            state_batch      = torch.stack([torch.tensor(s, dtype=torch.float32) for s in batch.state])\n",
    "            next_state_batch = torch.stack([torch.tensor(s, dtype=torch.float32) for s in batch.next_state])\n",
    "            action_batch     = torch.tensor(batch.action, dtype=torch.int64, device=DEVICE).unsqueeze(1)\n",
    "            reward_batch     = torch.tensor(batch.reward, dtype=torch.float32, device=DEVICE).unsqueeze(1)\n",
    "            done_batch       = torch.tensor(batch.done, dtype=torch.float32, device=DEVICE).unsqueeze(1)\n",
    "\n",
    "            # current Q-values\n",
    "            q_values = compute_q_rates(ann_model, state_batch)\n",
    "            current_q = q_values.gather(1, action_batch)\n",
    "\n",
    "            # Double DQN: select next action via online net\n",
    "            with torch.no_grad():\n",
    "                next_q_online = compute_q_rates(ann_model, next_state_batch)\n",
    "                next_actions  = next_q_online.argmax(dim=1, keepdim=True)\n",
    "\n",
    "                # evaluate with target net\n",
    "                next_q_target = compute_q_rates(ann_model_target, next_state_batch)\n",
    "                next_q        = next_q_target.gather(1, next_actions)\n",
    "\n",
    "                # build TD target, mask terminals\n",
    "                td_target = reward_batch + GAMMA * (1 - done_batch) * next_q\n",
    "\n",
    "            # loss & optimize\n",
    "            loss = criterion(current_q, td_target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # sync target network periodically\n",
    "    if ep % TARGET_SYNC == 0:\n",
    "        ann_model_target.load_state_dict(ann_model.state_dict())\n",
    "\n",
    "    print(f\"Episode {ep:03d}  Reward: {total_reward:.1f}  Epsilon: {eps:.3f}\")\n",
    "    \n",
    "    # save model every 10 episodes\n",
    "    if ep % 10 == 0:\n",
    "        torch.save(ann_model, f\"ann_q_net_finetuned.pth\")\n",
    "        torch.save(ann_model_target, f\"ann_q_net_finetuned_target.pth\")\n",
    "        print(f\"Saved model at episode {ep}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a34b6f9",
   "metadata": {},
   "source": [
    "# finetune SNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2016672d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ReplayBuffer.__init__() got an unexpected keyword argument 'capacity'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m env \u001b[38;5;241m=\u001b[39m VecFrameStack(env, n_stack\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     24\u001b[0m ε_start, ε_end, ε_decay \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m1000000\u001b[39m\n\u001b[0;32m---> 25\u001b[0m buffer \u001b[38;5;241m=\u001b[39m \u001b[43mReplayBuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcapacity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Create a target network for more stable TD\u001b[39;00m\n\u001b[1;32m     28\u001b[0m target_snn \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(fused_snn)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n",
      "\u001b[0;31mTypeError\u001b[0m: ReplayBuffer.__init__() got an unexpected keyword argument 'capacity'"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from spikingjelly.clock_driven import functional as sf_func\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "\n",
    "# ─── Hyperparameters ────────────────────────────────────────────────────────────\n",
    "ENV_ID         = \"PongNoFrameskip-v4\"\n",
    "NUM_EPISODES   = 500\n",
    "TIME_STEPS     = 20       # SNN ticks per frame\n",
    "GAMMA          = 0.99\n",
    "LR             = 1e-4\n",
    "TARGET_SYNC    = 10       # episodes between syncing target network\n",
    "BUFFER_SIZE    = 100_000\n",
    "BATCH_SIZE     = 32\n",
    "MIN_REPLAY     = 1_000    # start training after this many transitions\n",
    "EPS_START      = 1.0\n",
    "EPS_END        = 0.1\n",
    "EPS_DECAY      = 100_000  # frames over which epsilon decays\n",
    "DEVICE         = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ─── Environment setup ──────────────────────────────────────────────────────────\n",
    "env = make_atari_env(ENV_ID, n_envs=1, seed=0)\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "def unwrap(obs_tuple):\n",
    "    # unwrap VecEnv: obs_tuple is ([frames], infos)\n",
    "    return obs_tuple[0]\n",
    "\n",
    "# ─── Load & clone networks ──────────────────────────────────────────────────────\n",
    "fused_snn = torch.load(\"path/to/fused_snn.pth\", map_location=DEVICE)\n",
    "fused_snn.to(DEVICE)\n",
    "target_snn = copy.deepcopy(fused_snn).to(DEVICE)\n",
    "target_snn.eval()\n",
    "\n",
    "# ─── Replay buffer ──────────────────────────────────────────────────────────────\n",
    "Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "replay_buffer = deque(maxlen=BUFFER_SIZE)\n",
    "\n",
    "# ─── Optimizer & loss ───────────────────────────────────────────────────────────\n",
    "optimizer = optim.Adam(fused_snn.parameters(), lr=LR)\n",
    "criterion = nn.SmoothL1Loss()   # Huber loss\n",
    "\n",
    "# ─── Epsilon schedule ───────────────────────────────────────────────────────────\n",
    "def epsilon_by_frame(frame_idx):\n",
    "    return EPS_END + (EPS_START - EPS_END) * np.exp(-1.0 * frame_idx / EPS_DECAY)\n",
    "\n",
    "# ─── Helper: compute Q‐rates for a batch of observations ─────────────────────────\n",
    "def compute_q_rates(net, obs_batch):\n",
    "    \"\"\"\n",
    "    obs_batch: Tensor of shape (B, H, W, C), values in [0,255]\n",
    "    returns: Tensor of shape (B, action_dim)\n",
    "    \"\"\"\n",
    "    qs = []\n",
    "    for obs in obs_batch:\n",
    "        x = obs.permute(2, 0, 1).unsqueeze(0).to(DEVICE) / 255.0\n",
    "        sf_func.reset_net(net)\n",
    "        out_sum = torch.zeros((1, action_dim), device=DEVICE)\n",
    "        for _ in range(TIME_STEPS):\n",
    "            out_sum += net(x)\n",
    "        qs.append(out_sum.div_(TIME_STEPS))\n",
    "    return torch.cat(qs, dim=0)\n",
    "\n",
    "# ─── Pre-fill replay buffer with random play ────────────────────────────────────\n",
    "obs = unwrap(env.reset())\n",
    "for _ in range(MIN_REPLAY):\n",
    "    action = env.action_space.sample()\n",
    "    next_obs, reward, done, _ = env.step([action])\n",
    "    next_obs, reward, done = next_obs[0], reward[0], done[0]\n",
    "    replay_buffer.append(Transition(obs, action, reward, next_obs, done))\n",
    "    obs = next_obs if not done else unwrap(env.reset())\n",
    "\n",
    "# ─── Main training loop ─────────────────────────────────────────────────────────\n",
    "frame_idx = 0\n",
    "for ep in range(1, NUM_EPISODES + 1):\n",
    "    obs = unwrap(env.reset())\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    # reset spiking states\n",
    "    sf_func.reset_net(fused_snn)\n",
    "    sf_func.reset_net(target_snn)\n",
    "\n",
    "    while not done:\n",
    "        frame_idx += 1\n",
    "        eps = epsilon_by_frame(frame_idx)\n",
    "\n",
    "        # ε-greedy action selection\n",
    "        if random.random() < eps:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_rate = compute_q_rates(fused_snn, torch.tensor(obs[None], dtype=torch.float32))\n",
    "            action = q_rate.argmax(dim=1).item()\n",
    "\n",
    "        # step environment\n",
    "        next_obs, reward, done, _ = env.step([action])\n",
    "        next_obs, reward, done = next_obs[0], reward[0], done[0]\n",
    "        replay_buffer.append(Transition(obs, action, reward, next_obs, done))\n",
    "        obs = next_obs\n",
    "        total_reward += reward\n",
    "\n",
    "        # once we have enough samples, perform a training step\n",
    "        if len(replay_buffer) >= MIN_REPLAY:\n",
    "            transitions = random.sample(replay_buffer, BATCH_SIZE)\n",
    "            batch = Transition(*zip(*transitions))\n",
    "\n",
    "            state_batch      = torch.stack([torch.tensor(s, dtype=torch.float32) for s in batch.state])\n",
    "            next_state_batch = torch.stack([torch.tensor(s, dtype=torch.float32) for s in batch.next_state])\n",
    "            action_batch     = torch.tensor(batch.action, dtype=torch.int64, device=DEVICE).unsqueeze(1)\n",
    "            reward_batch     = torch.tensor(batch.reward, dtype=torch.float32, device=DEVICE).unsqueeze(1)\n",
    "            done_batch       = torch.tensor(batch.done, dtype=torch.float32, device=DEVICE).unsqueeze(1)\n",
    "\n",
    "            # current Q-values\n",
    "            q_values = compute_q_rates(fused_snn, state_batch)\n",
    "            current_q = q_values.gather(1, action_batch)\n",
    "\n",
    "            # Double DQN: select next action via online net\n",
    "            with torch.no_grad():\n",
    "                next_q_online = compute_q_rates(fused_snn, next_state_batch)\n",
    "                next_actions  = next_q_online.argmax(dim=1, keepdim=True)\n",
    "\n",
    "                # evaluate with target net\n",
    "                next_q_target = compute_q_rates(target_snn, next_state_batch)\n",
    "                next_q        = next_q_target.gather(1, next_actions)\n",
    "\n",
    "                # build TD target, mask terminals\n",
    "                td_target = reward_batch + GAMMA * (1 - done_batch) * next_q\n",
    "\n",
    "            # loss & optimize\n",
    "            loss = criterion(current_q, td_target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # sync target network periodically\n",
    "    if ep % TARGET_SYNC == 0:\n",
    "        target_snn.load_state_dict(fused_snn.state_dict())\n",
    "\n",
    "    print(f\"Episode {ep:03d}  Reward: {total_reward:.1f}  Epsilon: {eps:.3f}\")\n",
    "\n",
    "# ─── Save final weights ─────────────────────────────────────────────────────────\n",
    "torch.save(fused_snn.state_dict(), \"fused_snn_dqn_final.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e0b171",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca16be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spikingjelly.clock_driven import functional as sf_func\n",
    "\n",
    "print(\"Evaluating SNN with rate coding...\")\n",
    "episodes   = 5\n",
    "time_steps = 20  # how many SNN ticks per frame\n",
    "rewards    = []\n",
    "spike_outputs = []\n",
    "\n",
    "# Make sure your network is in eval mode\n",
    "fused_snn.eval()\n",
    "\n",
    "for ep in range(episodes):\n",
    "    obs    = env.reset()\n",
    "    obs       = obs[0]    # unwrap VecEnv\n",
    "    \n",
    "    done      = False\n",
    "    total_reward = 0\n",
    "    steps_per_episode = 0\n",
    "    sf_func.reset_net(fused_snn)\n",
    "    \n",
    "    while done == False:\n",
    "        # preprocess frame to [1,4,84,84]\n",
    "        x = (\n",
    "            torch.tensor(obs, dtype=torch.float32)\n",
    "                 .permute(2, 0, 1)\n",
    "                 .unsqueeze(0)\n",
    "                 .to(device)\n",
    "            # / 255.0\n",
    "        )\n",
    "\n",
    "        # reset all LIF states before rate‐coding loop\n",
    "        sf_func.reset_net(fused_snn)\n",
    "\n",
    "        # accumulate outputs over time_steps\n",
    "        out_sum = torch.zeros(\n",
    "            (1, fused_snn.action_space.n), device=device\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for t in range(time_steps):\n",
    "                out = fused_snn(x)   # returns spike‐counts or membrane outputs for this tick\n",
    "                spike_outputs.append(out.detach().cpu().numpy())\n",
    "                out_sum += out\n",
    "\n",
    "        # compute rate‐coded Q values\n",
    "        q_rate = out_sum / float(time_steps)\n",
    "        # print(q_rate)\n",
    "        action = q_rate.argmax(dim=1).item()\n",
    "\n",
    "        # step the environment\n",
    "        next_obs, reward, done, info = env.step([action])\n",
    "        done   = done[0]\n",
    "        reward = reward[0]\n",
    "        obs    = next_obs[0]\n",
    "\n",
    "        total_reward += reward\n",
    "        steps_per_episode += 1\n",
    "\n",
    "    rewards.append(total_reward)\n",
    "    print(f\"Episode {ep+1} reward: {total_reward}, steps: {steps_per_episode}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sbzoo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

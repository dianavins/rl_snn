{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8da8d5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import gymnasium as gym\n",
    "import snntorch as snn\n",
    "from snntorch import functional as SF\n",
    "from snntorch import spikeplot as splt\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv, VecFrameStack\n",
    "\n",
    "from spikingjelly.clock_driven import ann2snn, functional\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14cfb6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the ANN model (update for your environment)\n",
    "ann_model_path = \"/PongNoFrameskip-v4.zip\"\n",
    "\n",
    "# Determine device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create Atari Pong evaluation environment\n",
    "env = make_atari_env(\"PongNoFrameskip-v4\", n_envs=1, seed=0)\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "video_folder = '/Volumes/export/isn/diana/bindsnet/examples/pong/logs/videos/'  # Folder to save videos\n",
    "video_length = 2000  # Length of the recorded video (in timesteps)\n",
    "env = VecVideoRecorder(env, video_folder,\n",
    "                     record_video_trigger=lambda x: x == 0,  # Record starting from the first step\n",
    "                     video_length=video_length,\n",
    "                     name_prefix=f\"PongNoFrameskip-v4-SNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae88c49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk/diana/miniconda3/envs/sbzoo/lib/python3.10/site-packages/stable_baselines3/common/save_util.py:167: UserWarning: Could not deserialize object learning_rate. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: 'bytes' object cannot be interpreted as an integer\n",
      "  warnings.warn(\n",
      "/local_disk/diana/miniconda3/envs/sbzoo/lib/python3.10/site-packages/stable_baselines3/common/save_util.py:167: UserWarning: Could not deserialize object lr_schedule. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: 'bytes' object cannot be interpreted as an integer\n",
      "  warnings.warn(\n",
      "/local_disk/diana/miniconda3/envs/sbzoo/lib/python3.10/site-packages/stable_baselines3/common/save_util.py:167: UserWarning: Could not deserialize object exploration_schedule. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: 'bytes' object cannot be interpreted as an integer\n",
      "  warnings.warn(\n",
      "/local_disk/diana/miniconda3/envs/sbzoo/lib/python3.10/site-packages/stable_baselines3/common/vec_env/patch_gym.py:95: UserWarning: You loaded a model that was trained using OpenAI Gym. We strongly recommend transitioning to Gymnasium by saving that model again.\n",
      "  warnings.warn(\n",
      "/local_disk/diana/miniconda3/envs/sbzoo/lib/python3.10/site-packages/stable_baselines3/common/base_class.py:773: UserWarning: You are probably loading a DQN model saved with SB3 < 2.4.0, we truncated the optimizer state so you can save the model again to avoid issues in the future (see https://github.com/DLR-RM/stable-baselines3/pull/1963 for more info). Original error: loaded state dict contains a parameter group that doesn't match the size of optimizer's group \n",
      "Note: the model should still work fine, this only a warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load models\n",
    "ann_model_path = \"/Volumes/export/isn/diana/rl-baselines3-zoo/logs/dqn/PongNoFrameskip-v4_1/PongNoFrameskip-v4.zip\"\n",
    "ann_model = DQN.load(ann_model_path, custom_objects={\"replay_buffer_class\": None, \"optimize_memory_usage\": False})\n",
    "snn_model = torch.load(\"snn_pong_q_net_full.pt\", weights_only=False, map_location=device)\n",
    "fused_snn = torch.load(\"fused_snn_pong.pt\", weights_only=False, map_location=device)\n",
    "target_snn = torch.load(\"fused_snn_pong.pt\", weights_only=False, map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90c034eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_net.features_extractor.cnn.0.weight    requires_grad=False\n",
      "q_net.features_extractor.cnn.0.bias      requires_grad=False\n",
      "q_net.features_extractor.cnn.2.weight    requires_grad=False\n",
      "q_net.features_extractor.cnn.2.bias      requires_grad=False\n",
      "q_net.features_extractor.cnn.4.weight    requires_grad=False\n",
      "q_net.features_extractor.cnn.4.bias      requires_grad=False\n",
      "q_net.features_extractor.linear.0.weight requires_grad=False\n",
      "q_net.features_extractor.linear.0.bias   requires_grad=False\n",
      "q_net.q_net.0.weight                     requires_grad=True\n",
      "q_net.q_net.0.bias                       requires_grad=True\n",
      "q_net_target.features_extractor.cnn.0.weight requires_grad=False\n",
      "q_net_target.features_extractor.cnn.0.bias requires_grad=False\n",
      "q_net_target.features_extractor.cnn.2.weight requires_grad=False\n",
      "q_net_target.features_extractor.cnn.2.bias requires_grad=False\n",
      "q_net_target.features_extractor.cnn.4.weight requires_grad=False\n",
      "q_net_target.features_extractor.cnn.4.bias requires_grad=False\n",
      "q_net_target.features_extractor.linear.0.weight requires_grad=False\n",
      "q_net_target.features_extractor.linear.0.bias requires_grad=False\n",
      "q_net_target.q_net.0.weight              requires_grad=True\n",
      "q_net_target.q_net.0.bias                requires_grad=True\n"
     ]
    }
   ],
   "source": [
    "# 1) Freeze everything in feature extractor\n",
    "features_extractor = ann_model.policy.q_net.features_extractor\n",
    "for p in features_extractor.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "target_features_extractor = ann_model.policy.q_net_target.features_extractor\n",
    "for p in target_features_extractor.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# 3) Verify\n",
    "for name, p in ann_model.policy.named_parameters():\n",
    "    print(f\"{name:40s} requires_grad={p.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b610f67f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_model = DQN.load(ann_model_path, custom_objects={\"replay_buffer_class\": None, \"optimize_memory_usage\": False})\n",
    "\n",
    "# glorot initialize q network\n",
    "nn.init.xavier_normal_(ann_model.policy.q_net.q_net[0].weight)\n",
    "nn.init.zeros_(ann_model.policy.q_net.q_net[0].bias)\n",
    "nn.init.xavier_normal_(ann_model.policy.q_net_target.q_net[0].weight)\n",
    "nn.init.zeros_(ann_model.policy.q_net_target.q_net[0].bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82d702b",
   "metadata": {},
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f68333a",
   "metadata": {},
   "source": [
    "# finetune ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2fd0e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk/diana/miniconda3/envs/sbzoo/lib/python3.10/site-packages/stable_baselines3/common/save_util.py:167: UserWarning: Could not deserialize object learning_rate. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: 'bytes' object cannot be interpreted as an integer\n",
      "  warnings.warn(\n",
      "/local_disk/diana/miniconda3/envs/sbzoo/lib/python3.10/site-packages/stable_baselines3/common/save_util.py:167: UserWarning: Could not deserialize object lr_schedule. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: 'bytes' object cannot be interpreted as an integer\n",
      "  warnings.warn(\n",
      "/local_disk/diana/miniconda3/envs/sbzoo/lib/python3.10/site-packages/stable_baselines3/common/save_util.py:167: UserWarning: Could not deserialize object exploration_schedule. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: 'bytes' object cannot be interpreted as an integer\n",
      "  warnings.warn(\n",
      "/local_disk/diana/miniconda3/envs/sbzoo/lib/python3.10/site-packages/stable_baselines3/common/vec_env/patch_gym.py:95: UserWarning: You loaded a model that was trained using OpenAI Gym. We strongly recommend transitioning to Gymnasium by saving that model again.\n",
      "  warnings.warn(\n",
      "/local_disk/diana/miniconda3/envs/sbzoo/lib/python3.10/site-packages/stable_baselines3/common/base_class.py:773: UserWarning: You are probably loading a DQN model saved with SB3 < 2.4.0, we truncated the optimizer state so you can save the model again to avoid issues in the future (see https://github.com/DLR-RM/stable-baselines3/pull/1963 for more info). Original error: loaded state dict contains a parameter group that doesn't match the size of optimizer's group \n",
      "Note: the model should still work fine, this only a warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 121\u001b[0m\n\u001b[1;32m    118\u001b[0m done_batch       \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(batch\u001b[38;5;241m.\u001b[39mdone, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mDEVICE)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# current Q-values\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m q_values \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_q_rates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mann_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m current_q \u001b[38;5;241m=\u001b[39m q_values\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, action_batch)\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# Double DQN: select next action via online net\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 67\u001b[0m, in \u001b[0;36mcompute_q_rates\u001b[0;34m(net, obs_batch)\u001b[0m\n\u001b[1;32m     65\u001b[0m     x \u001b[38;5;241m=\u001b[39m obs\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(DEVICE) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n\u001b[1;32m     66\u001b[0m     sf_func\u001b[38;5;241m.\u001b[39mreset_net(net)\n\u001b[0;32m---> 67\u001b[0m     qs\u001b[38;5;241m.\u001b[39mappend(\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(qs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/local_disk/diana/miniconda3/envs/sbzoo/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/local_disk/diana/miniconda3/envs/sbzoo/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/local_disk/diana/miniconda3/envs/sbzoo/lib/python3.10/site-packages/stable_baselines3/dqn/policies.py:66\u001b[0m, in \u001b[0;36mQNetwork.forward\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs: PyTorchObs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     60\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03m    Predict the q-values.\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m    :param obs: Observation\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m    :return: The estimated Q-Value for each action.\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_net(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures_extractor\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/local_disk/diana/miniconda3/envs/sbzoo/lib/python3.10/site-packages/stable_baselines3/common/policies.py:130\u001b[0m, in \u001b[0;36mBaseModel.extract_features\u001b[0;34m(self, obs, features_extractor)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mextract_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs: PyTorchObs, features_extractor: BaseFeaturesExtractor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    123\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m    Preprocess the observation if needed and extract features.\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    :return: The extracted features\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m     preprocessed_obs \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_obs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize_images\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_images\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m features_extractor(preprocessed_obs)\n",
      "File \u001b[0;32m/local_disk/diana/miniconda3/envs/sbzoo/lib/python3.10/site-packages/stable_baselines3/common/preprocessing.py:92\u001b[0m, in \u001b[0;36mpreprocess_obs\u001b[0;34m(obs, observation_space, normalize_images)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 observation \u001b[38;5;241m=\u001b[39m transpose_obs\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m observation\n\u001b[0;32m---> 92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpreprocess_obs\u001b[39m(\n\u001b[1;32m     93\u001b[0m     obs: Union[th\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, th\u001b[38;5;241m.\u001b[39mTensor]],\n\u001b[1;32m     94\u001b[0m     observation_space: spaces\u001b[38;5;241m.\u001b[39mSpace,\n\u001b[1;32m     95\u001b[0m     normalize_images: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     96\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[th\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, th\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[1;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Preprocess observation to be to a neural network.\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    For images, it normalizes the values by dividing them by 255 (to have values in [0, 1])\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    :return:\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation_space, spaces\u001b[38;5;241m.\u001b[39mDict):\n\u001b[1;32m    109\u001b[0m         \u001b[38;5;66;03m# Do not modify by reference the original observation\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from spikingjelly.clock_driven import functional as sf_func\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "\n",
    "# ─── Hyperparameters ────────────────────────────────────────────────────────────\n",
    "ENV_ID         = \"PongNoFrameskip-v4\"\n",
    "NUM_EPISODES   = 500\n",
    "GAMMA          = 0.99\n",
    "LR             = 1e-4\n",
    "TARGET_SYNC    = 10       # episodes between syncing target network\n",
    "BUFFER_SIZE    = 100_000\n",
    "BATCH_SIZE     = 32\n",
    "MIN_REPLAY     = 1_000    # start training after this many transitions\n",
    "EPS_START      = 1.0\n",
    "EPS_END        = 0.1\n",
    "EPS_DECAY      = 100_000  # frames over which epsilon decays\n",
    "DEVICE         = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ─── Environment setup ──────────────────────────────────────────────────────────\n",
    "env = make_atari_env(ENV_ID, n_envs=1, seed=0)\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "def unwrap(obs_tuple):\n",
    "    # unwrap VecEnv: obs_tuple is ([frames], infos)\n",
    "    return obs_tuple[0]\n",
    "\n",
    "# ─── Load & clone networks ──────────────────────────────────────────────────────\n",
    "fine_tuned_ann_model_path = \"./ann_q_net_finetuned.pth\" # epsilon = 0.850\n",
    "fine_tuned_ann_target_model_path = \"./ann_q_net_finetuned_target.pth\" # epsilon = 0.850\n",
    "# ann_model = torch.load(fine_tuned_ann_model_path, weights_only=False, map_location=DEVICE)\n",
    "# ann_model_target = torch.load(fine_tuned_ann_target_model_path, weights_only=False, map_location=DEVICE)\n",
    "ann_model = DQN.load(ann_model_path, custom_objects={\"replay_buffer_class\": None, \"optimize_memory_usage\": False})\n",
    "ann_model_target = ann_model.q_net_target\n",
    "ann_model = ann_model.q_net\n",
    "\n",
    "# ─── Replay buffer ──────────────────────────────────────────────────────────────\n",
    "Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "replay_buffer = deque(maxlen=BUFFER_SIZE)\n",
    "\n",
    "# ─── Optimizer & loss ───────────────────────────────────────────────────────────\n",
    "optimizer = optim.Adam(ann_model.parameters(), lr=LR)\n",
    "criterion = nn.SmoothL1Loss()   # Huber loss\n",
    "\n",
    "# ─── Epsilon schedule ───────────────────────────────────────────────────────────\n",
    "def epsilon_by_frame(frame_idx):\n",
    "    return EPS_END + (EPS_START - EPS_END) * np.exp(-1.0 * frame_idx / EPS_DECAY)\n",
    "\n",
    "# ─── Helper: compute Q‐rates for a batch of observations ─────────────────────────\n",
    "def compute_q_rates(net, obs_batch):\n",
    "    \"\"\"\n",
    "    obs_batch: Tensor of shape (B, H, W, C), values in [0,255]\n",
    "    returns: Tensor of shape (B, action_dim)\n",
    "    \"\"\"\n",
    "    qs = []\n",
    "    for obs in obs_batch:\n",
    "        x = obs.permute(2, 0, 1).unsqueeze(0).to(DEVICE) / 255.0\n",
    "        sf_func.reset_net(net)\n",
    "        qs.append(net(x))\n",
    "    return torch.cat(qs, dim=0)\n",
    "\n",
    "# ─── Pre-fill replay buffer with random play ────────────────────────────────────\n",
    "obs = unwrap(env.reset())\n",
    "for _ in range(MIN_REPLAY):\n",
    "    action = env.action_space.sample()\n",
    "    next_obs, reward, done, _ = env.step([action])\n",
    "    next_obs, reward, done = next_obs[0], reward[0], done[0]\n",
    "    replay_buffer.append(Transition(obs, action, reward, next_obs, done))\n",
    "    obs = next_obs if not done else unwrap(env.reset())\n",
    "\n",
    "# ─── Main training loop ─────────────────────────────────────────────────────────\n",
    "frame_idx = 0\n",
    "for ep in range(1, NUM_EPISODES + 1):\n",
    "    obs = unwrap(env.reset())\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    # reset spiking states\n",
    "    sf_func.reset_net(ann_model)\n",
    "    sf_func.reset_net(ann_model_target)\n",
    "\n",
    "    while not done:\n",
    "        frame_idx += 1\n",
    "        eps = epsilon_by_frame(frame_idx)\n",
    "\n",
    "        # ε-greedy action selection\n",
    "        if random.random() < eps:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_rate = compute_q_rates(ann_model, torch.tensor(obs[None], dtype=torch.float32))\n",
    "            action = q_rate.argmax(dim=1).item()\n",
    "\n",
    "        # step environment\n",
    "        next_obs, reward, done, _ = env.step([action])\n",
    "        next_obs, reward, done = next_obs[0], reward[0], done[0]\n",
    "        replay_buffer.append(Transition(obs, action, reward, next_obs, done))\n",
    "        obs = next_obs\n",
    "        total_reward += reward\n",
    "\n",
    "        # once we have enough samples, perform a training step\n",
    "        if len(replay_buffer) >= MIN_REPLAY:\n",
    "            transitions = random.sample(replay_buffer, BATCH_SIZE)\n",
    "            batch = Transition(*zip(*transitions))\n",
    "\n",
    "            state_batch      = torch.stack([torch.tensor(s, dtype=torch.float32) for s in batch.state])\n",
    "            next_state_batch = torch.stack([torch.tensor(s, dtype=torch.float32) for s in batch.next_state])\n",
    "            action_batch     = torch.tensor(batch.action, dtype=torch.int64, device=DEVICE).unsqueeze(1)\n",
    "            reward_batch     = torch.tensor(batch.reward, dtype=torch.float32, device=DEVICE).unsqueeze(1)\n",
    "            done_batch       = torch.tensor(batch.done, dtype=torch.float32, device=DEVICE).unsqueeze(1)\n",
    "\n",
    "            # current Q-values\n",
    "            q_values = compute_q_rates(ann_model, state_batch)\n",
    "            current_q = q_values.gather(1, action_batch)\n",
    "\n",
    "            # Double DQN: select next action via online net\n",
    "            with torch.no_grad():\n",
    "                next_q_online = compute_q_rates(ann_model, next_state_batch)\n",
    "                next_actions  = next_q_online.argmax(dim=1, keepdim=True)\n",
    "\n",
    "                # evaluate with target net\n",
    "                next_q_target = compute_q_rates(ann_model_target, next_state_batch)\n",
    "                next_q        = next_q_target.gather(1, next_actions)\n",
    "\n",
    "                # build TD target, mask terminals\n",
    "                td_target = reward_batch + GAMMA * (1 - done_batch) * next_q\n",
    "\n",
    "            # loss & optimize\n",
    "            loss = criterion(current_q, td_target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # sync target network periodically\n",
    "    if ep % TARGET_SYNC == 0:\n",
    "        ann_model_target.load_state_dict(ann_model.state_dict())\n",
    "\n",
    "    print(f\"Episode {ep:03d}  Reward: {total_reward:.1f}  Epsilon: {eps:.3f}\")\n",
    "    \n",
    "    # save model every 10 episodes\n",
    "    if ep % 10 == 0:\n",
    "        torch.save(ann_model, f\"ann_q_net_finetuned.pth\")\n",
    "        torch.save(ann_model_target, f\"ann_q_net_finetuned_target.pth\")\n",
    "        print(f\"Saved model at episode {ep}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a34b6f9",
   "metadata": {},
   "source": [
    "# finetune SNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2016672d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ReplayBuffer.__init__() got an unexpected keyword argument 'capacity'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m env \u001b[38;5;241m=\u001b[39m VecFrameStack(env, n_stack\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     24\u001b[0m ε_start, ε_end, ε_decay \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m1000000\u001b[39m\n\u001b[0;32m---> 25\u001b[0m buffer \u001b[38;5;241m=\u001b[39m \u001b[43mReplayBuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcapacity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Create a target network for more stable TD\u001b[39;00m\n\u001b[1;32m     28\u001b[0m target_snn \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(fused_snn)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n",
      "\u001b[0;31mTypeError\u001b[0m: ReplayBuffer.__init__() got an unexpected keyword argument 'capacity'"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from spikingjelly.clock_driven import functional as sf_func\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "\n",
    "# ─── Hyperparameters ────────────────────────────────────────────────────────────\n",
    "ENV_ID         = \"PongNoFrameskip-v4\"\n",
    "NUM_EPISODES   = 500\n",
    "TIME_STEPS     = 20       # SNN ticks per frame\n",
    "GAMMA          = 0.99\n",
    "LR             = 1e-4\n",
    "TARGET_SYNC    = 10       # episodes between syncing target network\n",
    "BUFFER_SIZE    = 100_000\n",
    "BATCH_SIZE     = 32\n",
    "MIN_REPLAY     = 1_000    # start training after this many transitions\n",
    "EPS_START      = 1.0\n",
    "EPS_END        = 0.1\n",
    "EPS_DECAY      = 100_000  # frames over which epsilon decays\n",
    "DEVICE         = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ─── Environment setup ──────────────────────────────────────────────────────────\n",
    "env = make_atari_env(ENV_ID, n_envs=1, seed=0)\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "def unwrap(obs_tuple):\n",
    "    # unwrap VecEnv: obs_tuple is ([frames], infos)\n",
    "    return obs_tuple[0]\n",
    "\n",
    "# ─── Load & clone networks ──────────────────────────────────────────────────────\n",
    "fused_snn = torch.load(\"path/to/fused_snn.pth\", map_location=DEVICE)\n",
    "fused_snn.to(DEVICE)\n",
    "target_snn = copy.deepcopy(fused_snn).to(DEVICE)\n",
    "target_snn.eval()\n",
    "\n",
    "# ─── Replay buffer ──────────────────────────────────────────────────────────────\n",
    "Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "replay_buffer = deque(maxlen=BUFFER_SIZE)\n",
    "\n",
    "# ─── Optimizer & loss ───────────────────────────────────────────────────────────\n",
    "optimizer = optim.Adam(fused_snn.parameters(), lr=LR)\n",
    "criterion = nn.SmoothL1Loss()   # Huber loss\n",
    "\n",
    "# ─── Epsilon schedule ───────────────────────────────────────────────────────────\n",
    "def epsilon_by_frame(frame_idx):\n",
    "    return EPS_END + (EPS_START - EPS_END) * np.exp(-1.0 * frame_idx / EPS_DECAY)\n",
    "\n",
    "# ─── Helper: compute Q‐rates for a batch of observations ─────────────────────────\n",
    "def compute_q_rates(net, obs_batch):\n",
    "    \"\"\"\n",
    "    obs_batch: Tensor of shape (B, H, W, C), values in [0,255]\n",
    "    returns: Tensor of shape (B, action_dim)\n",
    "    \"\"\"\n",
    "    qs = []\n",
    "    for obs in obs_batch:\n",
    "        x = obs.permute(2, 0, 1).unsqueeze(0).to(DEVICE) / 255.0\n",
    "        sf_func.reset_net(net)\n",
    "        out_sum = torch.zeros((1, action_dim), device=DEVICE)\n",
    "        for _ in range(TIME_STEPS):\n",
    "            out_sum += net(x)\n",
    "        qs.append(out_sum.div_(TIME_STEPS))\n",
    "    return torch.cat(qs, dim=0)\n",
    "\n",
    "# ─── Pre-fill replay buffer with random play ────────────────────────────────────\n",
    "obs = unwrap(env.reset())\n",
    "for _ in range(MIN_REPLAY):\n",
    "    action = env.action_space.sample()\n",
    "    next_obs, reward, done, _ = env.step([action])\n",
    "    next_obs, reward, done = next_obs[0], reward[0], done[0]\n",
    "    replay_buffer.append(Transition(obs, action, reward, next_obs, done))\n",
    "    obs = next_obs if not done else unwrap(env.reset())\n",
    "\n",
    "# ─── Main training loop ─────────────────────────────────────────────────────────\n",
    "frame_idx = 0\n",
    "for ep in range(1, NUM_EPISODES + 1):\n",
    "    obs = unwrap(env.reset())\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    # reset spiking states\n",
    "    sf_func.reset_net(fused_snn)\n",
    "    sf_func.reset_net(target_snn)\n",
    "\n",
    "    while not done:\n",
    "        frame_idx += 1\n",
    "        eps = epsilon_by_frame(frame_idx)\n",
    "\n",
    "        # ε-greedy action selection\n",
    "        if random.random() < eps:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_rate = compute_q_rates(fused_snn, torch.tensor(obs[None], dtype=torch.float32))\n",
    "            action = q_rate.argmax(dim=1).item()\n",
    "\n",
    "        # step environment\n",
    "        next_obs, reward, done, _ = env.step([action])\n",
    "        next_obs, reward, done = next_obs[0], reward[0], done[0]\n",
    "        replay_buffer.append(Transition(obs, action, reward, next_obs, done))\n",
    "        obs = next_obs\n",
    "        total_reward += reward\n",
    "\n",
    "        # once we have enough samples, perform a training step\n",
    "        if len(replay_buffer) >= MIN_REPLAY:\n",
    "            transitions = random.sample(replay_buffer, BATCH_SIZE)\n",
    "            batch = Transition(*zip(*transitions))\n",
    "\n",
    "            state_batch      = torch.stack([torch.tensor(s, dtype=torch.float32) for s in batch.state])\n",
    "            next_state_batch = torch.stack([torch.tensor(s, dtype=torch.float32) for s in batch.next_state])\n",
    "            action_batch     = torch.tensor(batch.action, dtype=torch.int64, device=DEVICE).unsqueeze(1)\n",
    "            reward_batch     = torch.tensor(batch.reward, dtype=torch.float32, device=DEVICE).unsqueeze(1)\n",
    "            done_batch       = torch.tensor(batch.done, dtype=torch.float32, device=DEVICE).unsqueeze(1)\n",
    "\n",
    "            # current Q-values\n",
    "            q_values = compute_q_rates(fused_snn, state_batch)\n",
    "            current_q = q_values.gather(1, action_batch)\n",
    "\n",
    "            # Double DQN: select next action via online net\n",
    "            with torch.no_grad():\n",
    "                next_q_online = compute_q_rates(fused_snn, next_state_batch)\n",
    "                next_actions  = next_q_online.argmax(dim=1, keepdim=True)\n",
    "\n",
    "                # evaluate with target net\n",
    "                next_q_target = compute_q_rates(target_snn, next_state_batch)\n",
    "                next_q        = next_q_target.gather(1, next_actions)\n",
    "\n",
    "                # build TD target, mask terminals\n",
    "                td_target = reward_batch + GAMMA * (1 - done_batch) * next_q\n",
    "\n",
    "            # loss & optimize\n",
    "            loss = criterion(current_q, td_target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # sync target network periodically\n",
    "    if ep % TARGET_SYNC == 0:\n",
    "        target_snn.load_state_dict(fused_snn.state_dict())\n",
    "\n",
    "    print(f\"Episode {ep:03d}  Reward: {total_reward:.1f}  Epsilon: {eps:.3f}\")\n",
    "\n",
    "# ─── Save final weights ─────────────────────────────────────────────────────────\n",
    "torch.save(fused_snn.state_dict(), \"fused_snn_dqn_final.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e0b171",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca16be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spikingjelly.clock_driven import functional as sf_func\n",
    "\n",
    "print(\"Evaluating SNN with rate coding...\")\n",
    "episodes   = 5\n",
    "time_steps = 20  # how many SNN ticks per frame\n",
    "rewards    = []\n",
    "spike_outputs = []\n",
    "\n",
    "# Make sure your network is in eval mode\n",
    "fused_snn.eval()\n",
    "\n",
    "for ep in range(episodes):\n",
    "    obs    = env.reset()\n",
    "    obs       = obs[0]    # unwrap VecEnv\n",
    "    \n",
    "    done      = False\n",
    "    total_reward = 0\n",
    "    steps_per_episode = 0\n",
    "    sf_func.reset_net(fused_snn)\n",
    "    \n",
    "    while done == False:\n",
    "        # preprocess frame to [1,4,84,84]\n",
    "        x = (\n",
    "            torch.tensor(obs, dtype=torch.float32)\n",
    "                 .permute(2, 0, 1)\n",
    "                 .unsqueeze(0)\n",
    "                 .to(device)\n",
    "            # / 255.0\n",
    "        )\n",
    "\n",
    "        # reset all LIF states before rate‐coding loop\n",
    "        sf_func.reset_net(fused_snn)\n",
    "\n",
    "        # accumulate outputs over time_steps\n",
    "        out_sum = torch.zeros(\n",
    "            (1, fused_snn.action_space.n), device=device\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for t in range(time_steps):\n",
    "                out = fused_snn(x)   # returns spike‐counts or membrane outputs for this tick\n",
    "                spike_outputs.append(out.detach().cpu().numpy())\n",
    "                out_sum += out\n",
    "\n",
    "        # compute rate‐coded Q values\n",
    "        q_rate = out_sum / float(time_steps)\n",
    "        # print(q_rate)\n",
    "        action = q_rate.argmax(dim=1).item()\n",
    "\n",
    "        # step the environment\n",
    "        next_obs, reward, done, info = env.step([action])\n",
    "        done   = done[0]\n",
    "        reward = reward[0]\n",
    "        obs    = next_obs[0]\n",
    "\n",
    "        total_reward += reward\n",
    "        steps_per_episode += 1\n",
    "\n",
    "    rewards.append(total_reward)\n",
    "    print(f\"Episode {ep+1} reward: {total_reward}, steps: {steps_per_episode}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sbzoo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7f664dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import gymnasium as gym\n",
    "import snntorch as snn\n",
    "from snntorch import functional as SF\n",
    "from snntorch import spikeplot as splt\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "from stable_baselines3 import A2C\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8af21ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANN model loaded successfully.\n",
      "Collecting observations...\n",
      "Collected observations.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv, VecFrameStack\n",
    "\n",
    "from spikingjelly.clock_driven import ann2snn, functional\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Path to the ANN model (update for your environment)\n",
    "ann_model_path = \"/Volumes/export/isn/diana/rl-baselines3-zoo/logs/dqn/PongNoFrameskip-v4_1/PongNoFrameskip-v4.zip\"\n",
    "\n",
    "# Determine device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create Atari Pong evaluation environment\n",
    "env = make_atari_env(\"PongNoFrameskip-v4\", n_envs=1, seed=0)\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "video_folder = '/Volumes/export/isn/diana/bindsnet/examples/pong/logs/videos/'  # Folder to save videos\n",
    "video_length = 2000  # Length of the recorded video (in timesteps)\n",
    "env = VecVideoRecorder(env, video_folder,\n",
    "                     record_video_trigger=lambda x: x == 0,  # Record starting from the first step\n",
    "                     video_length=video_length,\n",
    "                     name_prefix=f\"PongNoFrameskip-v4-SNN\")\n",
    "\n",
    "# Collect observations using the ANN to estimate activation statistics\n",
    "ann_model = DQN.load(ann_model_path, custom_objects={\"replay_buffer_class\": None, \"optimize_memory_usage\": False})\n",
    "print(\"ANN model loaded successfully.\")\n",
    "\n",
    "obs = env.reset()\n",
    "observations = []\n",
    "print(\"Collecting observations...\")\n",
    "for _ in range(1000):\n",
    "    action, _states = ann_model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    observations.append(obs[0])\n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "print(\"Collected observations.\")\n",
    "\n",
    "# Convert list of numpy arrays to a torch tensor\n",
    "obs_array = np.stack(observations)                      # shape: [N, 84, 84, 4]\n",
    "obs_array = np.transpose(obs_array, (0, 3, 1, 2))       # shape: [N, 4, 84, 84]\n",
    "obs_tensor = torch.tensor(obs_array, dtype=torch.float32)\n",
    "dummy_labels = torch.zeros(len(obs_tensor), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24608e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloader created\n"
     ]
    }
   ],
   "source": [
    "# Wrap in TensorDataset to make it compatible with ann2snn.Converter\n",
    "obs_dataset = TensorDataset(obs_tensor, dummy_labels)\n",
    "loader = DataLoader(obs_dataset, batch_size=32, shuffle=False)\n",
    "print(\"Dataloader created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b4babfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting ANN to SNN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:00<00:00, 407.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNN model saved to snn_pong_q_net.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from spikingjelly.activation_based import model\n",
    "\n",
    "# Convert the Q-network of the ANN policy to a SNN\n",
    "print(\"Converting ANN to SNN...\")\n",
    "converter = ann2snn.Converter(dataloader=loader, mode=1.0 / 2)\n",
    "ann_q_net = ann_model.policy.q_net\n",
    "snn_q_net = converter(ann_q_net).to(device)\n",
    "\n",
    "# save snn_q_net to disk\n",
    "snn_q_net_path = \"snn_pong_q_net.pth\"\n",
    "torch.save(snn_q_net.state_dict(), snn_q_net_path)\n",
    "print(f\"SNN model saved to {snn_q_net_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e494b5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating SNN with rate coding...\n",
      "Episode 1 reward: 1.0, steps: 77\n",
      "Saving video to /Volumes/export/isn/diana/bindsnet/examples/pong/logs/videos/PongNoFrameskip-v4-SNN-step-0-to-step-2000.mp4\n",
      "MoviePy - Building video /Volumes/export/isn/diana/bindsnet/examples/pong/logs/videos/PongNoFrameskip-v4-SNN-step-0-to-step-2000.mp4.\n",
      "MoviePy - Writing video /Volumes/export/isn/diana/bindsnet/examples/pong/logs/videos/PongNoFrameskip-v4-SNN-step-0-to-step-2000.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready /Volumes/export/isn/diana/bindsnet/examples/pong/logs/videos/PongNoFrameskip-v4-SNN-step-0-to-step-2000.mp4\n",
      "Episode 2 reward: 21.0, steps: 1632\n"
     ]
    }
   ],
   "source": [
    "from spikingjelly.clock_driven import functional as sf_func\n",
    "\n",
    "print(\"Evaluating SNN with rate coding...\")\n",
    "episodes   = 2\n",
    "time_steps = 15  # how many SNN ticks per frame\n",
    "rewards    = []\n",
    "spike_outputs = []\n",
    "\n",
    "# Make sure your network is in eval mode\n",
    "snn_q_net.eval()\n",
    "\n",
    "for ep in range(episodes):\n",
    "    obs    = env.reset()\n",
    "    obs       = obs[0]    # unwrap VecEnv\n",
    "    done      = False\n",
    "    total_reward = 0\n",
    "    steps_per_episode = 0\n",
    "    sf_func.reset_net(snn_q_net)\n",
    "    \n",
    "    while done == False:\n",
    "        # preprocess frame to [1,4,84,84]\n",
    "        x = (\n",
    "            torch.tensor(obs, dtype=torch.float32)\n",
    "                 .permute(2, 0, 1)\n",
    "                 .unsqueeze(0)\n",
    "                 .to(device)\n",
    "            # / 255.0\n",
    "        )\n",
    "\n",
    "        # reset all LIF states before rate‐coding loop\n",
    "        sf_func.reset_net(snn_q_net)\n",
    "\n",
    "        # accumulate outputs over time_steps\n",
    "        out_sum = torch.zeros(\n",
    "            (1, ann_model.action_space.n), device=device\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for t in range(time_steps):\n",
    "                out = snn_q_net(x)   # returns spike‐counts or membrane outputs for this tick\n",
    "                spike_outputs.append(out.detach().cpu().numpy())\n",
    "                out_sum += out\n",
    "\n",
    "        # compute rate‐coded Q values\n",
    "        q_rate = out_sum / float(time_steps)\n",
    "        # print(q_rate)\n",
    "        action = q_rate.argmax(dim=1).item()\n",
    "\n",
    "        # step the environment\n",
    "        next_obs, reward, done, info = env.step([action])\n",
    "        done   = done[0]\n",
    "        reward = reward[0]\n",
    "        obs    = next_obs[0]\n",
    "\n",
    "        total_reward += reward\n",
    "        steps_per_episode += 1\n",
    "\n",
    "    rewards.append(total_reward)\n",
    "    print(f\"Episode {ep+1} reward: {total_reward}, steps: {steps_per_episode}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c928e89",
   "metadata": {},
   "source": [
    "# visualize an episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a7111c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode reward: 9.0, steps: 623\n"
     ]
    }
   ],
   "source": [
    "obs    = env.reset()\n",
    "obs       = obs[0]    # unwrap VecEnv\n",
    "done      = False\n",
    "total_reward = 0\n",
    "steps_per_episode = 0\n",
    "time_steps = 15  # how many SNN ticks per frame\n",
    "rewards    = []\n",
    "spike_outputs = []\n",
    "\n",
    "# Make sure your network is in eval mode\n",
    "snn_q_net.eval()\n",
    "sf_func.reset_net(snn_q_net)\n",
    "\n",
    "while done == False:\n",
    "    # preprocess frame to [1,4,84,84]\n",
    "    x = (\n",
    "        torch.tensor(obs, dtype=torch.float32)\n",
    "                .permute(2, 0, 1)\n",
    "                .unsqueeze(0)\n",
    "                .to(device)\n",
    "        # / 255.0\n",
    "    )\n",
    "\n",
    "    # reset all LIF states before rate‐coding loop\n",
    "    sf_func.reset_net(snn_q_net)\n",
    "\n",
    "    # accumulate outputs over time_steps\n",
    "    out_sum = torch.zeros(\n",
    "        (1, ann_model.action_space.n), device=device\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for t in range(time_steps):\n",
    "            out = snn_q_net(x)   # returns spike‐counts or membrane outputs for this tick\n",
    "            spike_outputs.append(out.detach().cpu().numpy())\n",
    "            out_sum += out\n",
    "\n",
    "    # compute rate‐coded Q values\n",
    "    q_rate = out_sum / float(time_steps)\n",
    "    # print(q_rate)\n",
    "    action = q_rate.argmax(dim=1).item()\n",
    "\n",
    "    # step the environment\n",
    "    next_obs, reward, done, info = env.step([action])\n",
    "    done   = done[0]\n",
    "    reward = reward[0]\n",
    "    obs    = next_obs[0]\n",
    "\n",
    "    total_reward += reward\n",
    "    steps_per_episode += 1\n",
    "\n",
    "rewards.append(total_reward)\n",
    "print(f\"Episode reward: {total_reward}, steps: {steps_per_episode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3327658",
   "metadata": {},
   "source": [
    "timesteps = 20 Evaluating SNN with rate coding...\n",
    "Episode 1 reward: 21.0, steps: 1642\n",
    "Episode 2 reward: 21.0, steps: 1626\n",
    "Episode 3 reward: 21.0, steps: 1644\n",
    "Episode 4 reward: 21.0, steps: 1622\n",
    "Episode 5 reward: 20.0, steps: 1733"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51e4320",
   "metadata": {},
   "source": [
    "# quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbe2cbe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QNetwork(\n",
      "  (features_extractor): NatureCNN(\n",
      "    (cnn): Sequential(\n",
      "      (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
      "      (1): Sequential(\n",
      "        (0): VoltageScaler(1.371911)\n",
      "        (1): IFNode(\n",
      "          v_threshold=1.0, v_reset=None, detach_reset=False\n",
      "          (surrogate_function): Sigmoid(alpha=4.0, spiking=True)\n",
      "        )\n",
      "        (2): VoltageScaler(0.728910)\n",
      "      )\n",
      "      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "      (3): Sequential(\n",
      "        (0): VoltageScaler(1.975725)\n",
      "        (1): IFNode(\n",
      "          v_threshold=1.0, v_reset=None, detach_reset=False\n",
      "          (surrogate_function): Sigmoid(alpha=4.0, spiking=True)\n",
      "        )\n",
      "        (2): VoltageScaler(0.506143)\n",
      "      )\n",
      "      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (5): Sequential(\n",
      "        (0): VoltageScaler(2.588178)\n",
      "        (1): IFNode(\n",
      "          v_threshold=1.0, v_reset=None, detach_reset=False\n",
      "          (surrogate_function): Sigmoid(alpha=4.0, spiking=True)\n",
      "        )\n",
      "        (2): VoltageScaler(0.386372)\n",
      "      )\n",
      "      (6): Flatten(start_dim=1, end_dim=-1)\n",
      "    )\n",
      "    (linear): Sequential(\n",
      "      (0): Linear(in_features=3136, out_features=512, bias=True)\n",
      "      (1): Sequential(\n",
      "        (0): VoltageScaler(1.575190)\n",
      "        (1): IFNode(\n",
      "          v_threshold=1.0, v_reset=None, detach_reset=False\n",
      "          (surrogate_function): Sigmoid(alpha=4.0, spiking=True)\n",
      "        )\n",
      "        (2): VoltageScaler(0.634844)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (q_net): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=6, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(snn_q_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d9a4eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QNetwork(\n",
      "  (features_extractor): NatureCNN(\n",
      "    (cnn): Sequential(\n",
      "      (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "      (3): ReLU()\n",
      "      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (5): ReLU()\n",
      "      (6): Flatten(start_dim=1, end_dim=-1)\n",
      "    )\n",
      "    (linear): Sequential(\n",
      "      (0): Linear(in_features=3136, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (q_net): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=6, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(ann_q_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63eea9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from hs_api.converter.cri_converter import Quantize_Network\n",
    "\n",
    "alpha = 4\n",
    "qn = Quantize_Network(w_alpha=alpha)\n",
    "net_quan = qn.quantize(ann_q_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e40bafa",
   "metadata": {},
   "source": [
    "# Hi-AER Spike Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71ec76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hs_api.converter.cri_converter import CRI_Converter\n",
    "\n",
    "input_layer = 1 #first pytorch layer that acts as synapses, indexing begins at 0 \n",
    "output_layer = 4 #last pytorch layer that acts as synapses\n",
    "snn_layers = 2 # number of snn layers \n",
    "input_shape = (1, 28, 28)\n",
    "backend = 'spikingjelly'\n",
    "v_threshold = qn.v_threshold\n",
    "    \n",
    "cn = CRI_Converter(num_steps = args.T,\n",
    "                   input_layer = input_layer, \n",
    "                   output_layer = output_layer, \n",
    "                   input_shape = input_shape,\n",
    "                   snn_layers = snn_layers,\n",
    "                   backend = backend,\n",
    "                   v_threshold = int(v_threshold))\n",
    "\n",
    "cn.layer_converter(net_quan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47271657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate the model\n",
    "\n",
    "config = {}\n",
    "config['neuron_type'] = \"I&F\"\n",
    "config['global_neuron_params'] = {}\n",
    "config['global_neuron_params']['v_thr'] = int(qn.v_threshold)\n",
    "\n",
    "softwareNetwork = CRI_network(dict(cn.axon_dict),\n",
    "                              connections=dict(cn.neuron_dict),\n",
    "                              config=config,target='simpleSim', \n",
    "                              outputs = cn.output_neurons,\n",
    "                              coreID=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sbzoo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
